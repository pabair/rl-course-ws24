{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander with Actor-Critic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we apply the actor-critic method to solve the cart pole environment.\n",
    "Note: The original source of the much of the code can be found [here](https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273) and a blog post about it [here](https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "from tqdm import notebook\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discount factor for future utilities\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "#number of episodes to run\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "#max steps per episode\n",
    "MAX_STEPS = 10000\n",
    "\n",
    "#score agent needs for environment to be solved\n",
    "SOLVED_SCORE = 195\n",
    "\n",
    "#device to run model on \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn our policy parameters\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in observations and outputs actions\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, action_space)\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        actions = self.output_layer(x)\n",
    "        action_probs = F.softmax(actions, dim=1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn state value\n",
    "class StateValueNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in state\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        state_value = self.output_layer(x)\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(network, state):\n",
    "\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    #use network to predict action probabilities\n",
    "    action_probs = network(state)\n",
    "    state = state.detach()\n",
    "    \n",
    "    #sample an action using the probability distribution\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    #return action\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_count = env.observation_space.shape[0]\n",
    "action_count = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Init network\n",
    "policy_network = PolicyNetwork(state_count, action_count).to(DEVICE)\n",
    "stateval_network = StateValueNetwork(state_count).to(DEVICE)\n",
    "\n",
    "#Init optimizer\n",
    "policy_optimizer = optim.SGD(policy_network.parameters(), lr=0.001)\n",
    "stateval_optimizer = optim.SGD(stateval_network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track scores\n",
    "scores = []\n",
    "\n",
    "#track recent scores\n",
    "recent_scores = deque(maxlen = 100)\n",
    "\n",
    "#run episodes\n",
    "for episode in notebook.tqdm(range(NUM_EPISODES)):\n",
    "    \n",
    "    #init variables\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    discount = 1\n",
    "    \n",
    "    #run episode, update online\n",
    "    for step in range(MAX_STEPS):\n",
    "        \n",
    "        #get action and log probability\n",
    "        action, log_prob = select_action(policy_network, state)\n",
    "        \n",
    "        #step with action\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        #update episode score\n",
    "        score += reward\n",
    "        \n",
    "        #get state value of current state\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        state_val = stateval_network(state_tensor)\n",
    "        \n",
    "        #get state value of next state\n",
    "        new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(DEVICE)        \n",
    "        new_state_val = stateval_network(new_state_tensor)\n",
    "        \n",
    "        #if terminal state, next state val is 0\n",
    "        if done:\n",
    "            new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        #calculate value function loss with MSE\n",
    "        val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "        val_loss *= discount\n",
    "        \n",
    "        #calculate policy loss\n",
    "        td_error = reward + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "        policy_loss = -log_prob * td_error\n",
    "        policy_loss *= discount\n",
    "        \n",
    "        #Backpropagate policy\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        #Backpropagate value\n",
    "        stateval_optimizer.zero_grad()\n",
    "        val_loss.backward()\n",
    "        stateval_optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        #move into new state, discount I\n",
    "        state = new_state\n",
    "        discount *= DISCOUNT_FACTOR\n",
    "    \n",
    "    #append episode score \n",
    "    scores.append(score)\n",
    "    recent_scores.append(score)\n",
    "    \n",
    "    #early stopping if we meet solved score goal\n",
    "    if np.array(recent_scores).mean() >= SOLVED_SCORE:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Training score of CartPole Actor-Critic TD(0)')\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1), np.array(scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the network to play some episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state, _ = env.reset()\n",
    "scores = []\n",
    "\n",
    "for _ in notebook.tqdm(range(50)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, lp = select_action(policy_network, state)\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        score += reward\n",
    "        state = new_state\n",
    "    scores.append(score)\n",
    "env.close()\n",
    "print(\"Mean reward earned:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
